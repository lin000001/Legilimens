# Copyright 2023 PKU-Alignment Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Evaluation of the safety of QA pairs generated by different models."""

import argparse
import json
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import torch
import time
import matplotlib.pyplot as plt
import numpy as np
from rich.console import Console
from rich.table import Table
import sys
sys.path.append('../../')
from tools.dataset import CommonDataset, OigfeatureDataset, PromptDataset, JailbreakDataset, CustomDataset
from torch.utils.data import Dataset  
import pandas as pd
from moderation import QAModeration
from tqdm import tqdm
MODEL_NAMES = ['alpaca-7b', 'alpaca-13b', 'vicuna-7b', 'gpt-3.5-turbo']


def print_table(column: list[str], data: list[list]) -> None:
	"""Print a table."""
	table = Table(show_header=True, header_style='bold magenta', show_lines=True)
	for col in column:
		table.add_column(col, justify='center', style='bold')

	for row in data:
		table.add_row(*row)

	console = Console()
	console.print(table)


def calculate_flagged_proportion_and_agreement(data: dict) -> dict:
	flagged_gpt4 = np.array([line['flagged']['gpt4'] for line in data], dtype=bool)
	flagged_moderation = np.array([line['flagged']['moderation'] for line in data], dtype=bool)
	flagged_human = np.array([line['flagged']['human'] for line in data], dtype=bool)

	return {
		'agreement/gpt4&moderation': np.mean(flagged_gpt4 == flagged_moderation),
		'agreement/gpt4&human': np.mean(flagged_gpt4 == flagged_human),
		'agreement/moderation&human': np.mean(flagged_moderation == flagged_human),
		'flagged_proportion/gpt4': flagged_gpt4.mean(),
		'flagged_proportion/moderation': flagged_moderation.mean(),
		'flagged_proportion/human': flagged_human.mean(),
	}


def plot_metrics(metrics: list[dict], output_dir: str) -> None:
	"""Plot metrics."""
	model_names = np.asarray([row['model_name'] for row in metrics])
	moderation = np.asarray([row['flagged_proportion/moderation'] for row in metrics])
	gpt4 = np.asarray([row['flagged_proportion/gpt4'] for row in metrics])
	human = np.asarray([row['flagged_proportion/human'] for row in metrics])
	bar_width = 0.25
	index = np.arange(len(moderation))
	_, ax = plt.subplots(figsize=(8, 6), dpi=150)  # pylint: disable=invalid-name
	ax.bar(
		index,
		1.0 - moderation,
		bar_width,
		label='QA-Moderation',
		color='#FF6D60',
		alpha=0.85,
		zorder=2,
	)
	ax.bar(
		index + bar_width,
		1.0 - gpt4,
		bar_width,
		label='GPT-4 Evaluation',
		color='#98D8AA',
		alpha=0.85,
		zorder=2,
	)
	ax.bar(
		index + 2 * bar_width,
		1.0 - human,
		bar_width,
		label='Human Evaluation',
		color='#6DA9E4',
		alpha=0.85,
		zorder=2,
	)
	plt.grid(axis='y', color='k', alpha=0.2, zorder=1)
	ax.set_xticks(index + bar_width)
	ax.set_xticklabels(model_names)
	ax.set_xlabel('Model')
	ax.set_ylabel('Proportion of Safe QA Pairs')
	ax.set_title('Safety Evaluation of Different Models')
	ax.set_yticks(np.arange(0.4, 1.1, 0.1))
	ax.axhline(y=1.0, color='k', linestyle='-.', alpha=0.5)
	ax.set_yticklabels([f'{i}%' for i in range(40, 110, 10)])
	ax.set_ylim(0.35, 1.03)
	plt.legend(bbox_to_anchor=(0.05, -0.3), loc='lower left')

	agreement_gpt4_moderation = np.asarray([row['agreement/gpt4&moderation'] for row in metrics])
	agreement_gpt4_human = np.asarray([row['agreement/gpt4&human'] for row in metrics])
	agreement_moderation_human = np.asarray([row['agreement/moderation&human'] for row in metrics])
	ax_twin = ax.twinx()
	ax_twin.plot(
		index + bar_width,
		agreement_gpt4_moderation,
		color='#FFA559',
		label='GPT-4 vs. QA-Moderation',
		linestyle='-.',
		marker='*',
		markersize=7,
		zorder=5,
	)
	ax_twin.plot(
		index + bar_width,
		agreement_gpt4_human,
		color='#FFB4B4',
		label='GPT-4 vs. Human',
		linestyle='--',
		marker='^',
		markersize=6,
		zorder=4,
	)
	ax_twin.plot(
		index + bar_width,
		agreement_moderation_human,
		color='#BA90C6',
		label='QA-Moderation vs. Human',
		linestyle=':',
		marker='s',
		markersize=6,
		zorder=3,
	)
	# ax_twin.legend(loc='outside upper right')
	ax_twin.set_yticks(np.arange(0.4, 1.1, 0.1))
	ax_twin.set_yticklabels([f'{i}%' for i in range(40, 110, 10)])
	ax_twin.set_ylim(0.35, 1.03)
	ax_twin.set_ylabel('Agreement Ratio')
	# fig.legend(bbox_to_anchor =(0.5,-0.27), loc='outside lower center', ncol=2)
	plt.legend(bbox_to_anchor=(0.95, -0.3), loc='lower right')
	plt.tight_layout()
	plt.savefig(os.path.join(output_dir, 'flagged-proportion.png'))


class Prompt_ResponseDataset(Dataset):
	def __init__(self, csv_file, labels_column):
		self.df = pd.read_csv(csv_file)
		self.prompts = self.df['prompt'].values
		self.response = self.df['response'].values
		self.labels = self.df[labels_column].values

	def __len__(self):
		return len(self.df)

	def __getitem__(self, idx):
		sample = {'prompts': self.prompts[idx], 'response': self.response[idx], 'labels': self.labels[idx] }
		return sample

def parse_arguments():
	"""Parse command-line arguments."""
	parser = argparse.ArgumentParser()
	parser.add_argument(
		'--model_path',
		type=str,
		# required=True,
		default= './pretrained_model/beaver-dam-7b',
		help='Path to the model.',
	)
	parser.add_argument(
		'--max_length',
		type=int,
		default=5120,
		help='The maximum sequence length of the model.',
	)
	
	parser.add_argument(
		'--output_dir',
		type=str,
		# required=True,
		default= '../result/beaver_dam_0713/',
		help='Where to store.',
	)
	parser.add_argument('--test_dataset', type=str, required=True)
	parser.add_argument('--dataset_path', type=str)
	args = parser.parse_args()
	args.output_dir += f'{args.test_dataset}/'
	os.makedirs(args.output_dir, exist_ok=True)
	if args.test_dataset == 'beaver':
		args.dataset_path = '../../test_data_subset/test_data_subset/beaver_split_jailbreak_test_2000.csv'
	elif args.test_dataset == 'jailbreak_a':
		args.dataset_path = '../../test_data_subset/test_data_subset/jailbreak_2000.csv'
	elif args.test_dataset == 'oig':
		args.dataset_path = '../../test_data_subset/test_data_subset/final_test_oig_simple.csv'
	elif args.test_dataset == 'jigsaw':
		args.dataset_path = '../../test_data_subset/test_data_subset/jigsaw_test_2000.csv'
	elif args.test_dataset == 'hatexplain':
		args.dataset_path = '../../test_data_subset/test_data_subset/hx_2000.csv'
		# args.dataset_path = '/home/xtjh/llm/legilimens/test_data_subset/token2000_test.csv'
	elif args.test_dataset == 'mhs':
		args.dataset_path = '../../test_data_subset/test_data_subset/MHS_test_0dot5_2000.csv'
	elif args.test_dataset == 'qa_task':
		args.dataset_path = '../../test_data_subset/test_data_subset/QA_task_restrict_2000.csv'
	elif args.test_dataset == 'qa_task_adv':
		args.dataset_path = '../../test_data_subset/test_data_subset/BAG_adv_test_2000.csv'
	else:
		raise NotImplementedError
	return args

def to_binary(dataset_name, labels):
	if dataset_name == 'oig':
		bool_labels = labels == 'needs intervention'
	elif dataset_name == 'jigsaw':
		bool_labels = labels != 'innocent'
	elif dataset_name == 'beaver' or 'jailbreak_a':
		bool_labels = ~labels
	# import pdb;pdb.set_trace()
	binary_labels = np.array(bool_labels, dtype=np.int64)
	return binary_labels
	
def load_dataset(dataset_name, dataset_path):
	dataset_name = dataset_name.lower()
	if dataset_name == "beaver":  
		# trainset = PromptDataset(dataset_path, 'text', 'label')  
		trainset = Prompt_ResponseDataset(dataset_path, 'label')
		binary_label = to_binary(dataset_name, trainset.labels)
		return trainset.prompts, trainset.response, trainset.labels  
	elif dataset_name == "oig":  
		trainset = OigfeatureDataset(dataset_path, 'Input', 'Output')  
		binary_label = to_binary(dataset_name, trainset.labels)
		return trainset.features, None, binary_label
	elif dataset_name == "jigsaw":  
		trainset = CustomDataset(dataset_path, 'comment_text', 'label')
		binary_label = to_binary(dataset_name, trainset.labels)
		return trainset.features, None, binary_label
	elif dataset_name == "jailbreak_a":
		
		trainset = JailbreakDataset(dataset_path, 'jailbreak_prompt', 'response','label')
		binary_label = to_binary(dataset_name, trainset.labels)
		return trainset.features, trainset.response, trainset.labels 
	elif dataset_name == "qa_task":
		trainset = JailbreakDataset(dataset_path, 'prompt', 'response','label')
		return trainset.features, trainset.response, trainset.labels
	elif dataset_name == "qa_task_adv":
		trainset = JailbreakDataset(dataset_path, 'prompt', 'response','label')
		return trainset.features, trainset.response, trainset.labels
	elif dataset_name == "hatexplain":
		trainset = PromptDataset(dataset_path, 'text', 'label')
		# binary_label = to_binary(dataset_name, trainset.labels)
		return trainset.features, None, trainset.labels
	elif dataset_name == "mhs":
		trainset = PromptDataset(dataset_path, 'text', 'label')
		# binary_label = to_binary(dataset_name, trainset.labels)
		return trainset.features, None, trainset.labels
	else:  
		raise NotImplementedError 

def main() -> None:
	import pandas as pd
	args = parse_arguments()
	from sklearn.metrics import confusion_matrix, roc_auc_score
	os.makedirs(args.output_dir, exist_ok=True)
	prompts, responses, labels = load_dataset(args.test_dataset, args.dataset_path)
	model = QAModeration.from_pretrained(
		args.model_path,
		model_max_length=args.max_length,
		device_map='auto',
	)
	avg_time = 0
	

	# if not responses: 
	if args.test_dataset == 'oig':
		responses = prompts
	if args.test_dataset == 'jigsaw':
		responses = prompts
	if args.test_dataset == 'hatexplain':
		responses = prompts
	if args.test_dataset == 'mhs':
		responses = prompts
	# batch_begin = time.time()
	input_time = []
	token_length =[]
	# predictions = []
	predictions = model.predict(
		question=prompts,
		answer=responses,
		batch_size=2,
		return_bool=True,
		threshold=0.5,
	)

	
	with open(f'{args.output_dir}/predictions_{args.test_dataset}_test.json', 'w', encoding='utf-8') as f:
		json.dump(predictions, f, ensure_ascii=False, indent=4) 
		json.dump({"groundtruth": labels.tolist()}, f, ensure_ascii=False, indent=4)	
	binary_prediction = []
	for sample in predictions:
		if sample['flagged']:
			binary_prediction.append(1)
		else:
			binary_prediction.append(0)
	binary_prediction = np.array(binary_prediction)
	tn, fp, fn, tp = confusion_matrix(labels, binary_prediction).ravel()  
	auc_score = roc_auc_score(labels, binary_prediction)  
	print(f'AUC: {auc_score}')
	fpr = fp / (fp + tn)  
	fnr = fn / (fn + tp)
	acc = np.sum(binary_prediction==labels)*1.0/labels.shape[0]
	print('acc is:', acc)
	print('average time is:', avg_time)
	print('FPR and FNR:', fpr, fnr)


if __name__ == '__main__':
	# os.environ['CUDA_VISIBLE_DEVICES'] = '2,4'
	# os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
	main()
